{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fellahabdelnour13/autoencoders-with-mnist-dataset?scriptVersionId=192371761\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<h1 style=\"font-family:verdana;\"> Overview </h1>\n\n1. [What are auto-encoders ?](#what_are_auto_encoders)\n2. [Necessary Packages](#necessary_packages)\n3. [Constants](#constants)\n4. [Reproducibility](#reproducibility)\n5. [Data Loading](#data_loading)\n6. [Visualization](#visualization)\n7. [Utils](#utils)\n8. [Simple Auto-encoders](#simple_auto_encoders)\n     1. [The Architecture](#1_the_architecture)\n     2. [Training](#1_training)\n     3. [Evaluation](#1_evaluation)\n     4. [Auto-encoders Vs PCA (Visualization)](#1_autoencoders_vs_pca_visualization)\n     5. [Auto-encoders Vs PCA (Features Quality)](#1_autoencoders_vs_pca_features_quality)\n     6. [Retraining With a bigger bottleneck size](#1_retraining_with_bigger_bottleneck_size)\n9. [Convolutiona Auto-Encoders](#convolutional_auto_encoders)\n    1. [The architecture](#2_the_architecture)\n    2. [Training](#2_training)\n    3. [Evaluation](#2_evaluation)\n10. [Denoising Auto-Encoders](#denoising_auto_encoders)\n    1. [Adding noise to the input](#3_adding_noise_to_the_input)\n    2. [Training](#3_training)\n    3. [Evaluation](#3_evaluation)\n11. [Variational Autoencoders](#variational_auto_encoders)\n    1. [The architecture](#4_the_architecture)\n    2. [The loss function](#4_the_loss_function)\n    3. [Training](#4_training)\n    4. [Evaluation](#4_evaluation)\n    5. [Generating Synthetic Images](#4_generating_synthetic_data)\n12. [Conditional Variational Autoencoders](#conditional_variational_auto_encoders)\n    1. [The architecture](#5_the_architecture)\n    2. [Training](#5_training)\n    3. [Evaluation](#5_evaluation)\n    4. [Generating Synthetic Images](#5_generating_synthetic_data)\n13. [Conclusion](#conclusion)\n14. [Thank you :)](#thank_you)","metadata":{}},{"cell_type":"markdown","source":"<div id=\"what_are_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> What are auto-encoders ? </h1>\n</div>\n\n<div style=\"font-size: 1rem;font-family:verdana;\" >\n    Autoencoders are a type of artificial neural network used to learn efficient codings of unlabeled data. They are primarily used for dimensionality reduction and feature learning. The structure of an autoencoder consists of two main parts: the encoder, which compresses the input into a latent-space representation, and the decoder, which reconstructs the input from this representation.\n</div> <br/>\n\n\n<div style=\"font-size:1.125rem;font-family:verdana;font-weight:bold;\" >\nTypes of Autoencoders:\n</div><br/>\n    \n1. **Simple Autoencoders** : <br/>\n    * *Architecture* : Consist of fully connected layers.   \n    * *Use cases* : Dimensionality reduction, noise reduction, anomaly detection.\n2. **Convolutional Autoencoders** : <br/>\n    * *Architecture* : Use convolutional layers for encoding and decoding. \n    * *Use cases* : Image processing tasks such as denoising, inpainting, and super-resolution.\n3. **Denoising Autoencoders**\n    * *Architecture* : Can be either fully connected or convolutional, but trained to remove noise from input data.   \n    * *Use cases* : Image denoising, improving robustness of representations.\n4. **Variational Autoencoders (VAEs)**\n    * *Architecture* : Probabilistic approach to autoencoders that imposes a distribution on the latent space.  \n    * *Use cases* : Generative tasks, semi-supervised learning, anomaly detection.\n5. **Conditional Variational Autoencoders (VAEs)**\n    * *Architecture* : Extend VAEs by conditioning the generation process on additional information, such as labels.\n    * *Use cases* : Conditional image generation, data augmentation.\n    \n<div style=\"font-size: 1rem;font-family:verdana;\" >\n    For more details you can refer to :\n</div>\n\n* [Introduction To Autoencoders.](https://towardsdatascience.com/introduction-to-autoencoders-7a47cf4ef14b)\n* [Understanding Variational Autoencoders (VAEs).](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n* [Conditional Variational Autoencoders with Learnable Conditional Embeddings.](https://towardsdatascience.com/conditional-variational-autoencoders-with-learnable-conditional-embeddings-e22ee5359a2a)","metadata":{}},{"cell_type":"markdown","source":"<div id=\"necessary_packages\" >\n    <h1 style=\"font-family:verdana;\"> Necessary Packages </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport seaborn as sns\nimport random\nimport warnings\nfrom torch import nn,Tensor,optim\nfrom torch.utils.data import DataLoader,Dataset,default_collate\nfrom typing import Callable,Optional,Any,Literal\nfrom tqdm.notebook import tqdm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline \nfrom functools import reduce\nfrom torchinfo import summary\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:10.942534Z","iopub.execute_input":"2024-08-06T00:42:10.94312Z","iopub.status.idle":"2024-08-06T00:42:16.716506Z","shell.execute_reply.started":"2024-08-06T00:42:10.943088Z","shell.execute_reply":"2024-08-06T00:42:16.715466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter(action=\"ignore\",category=FutureWarning)\nsns.set_style(\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.718388Z","iopub.execute_input":"2024-08-06T00:42:16.719038Z","iopub.status.idle":"2024-08-06T00:42:16.723829Z","shell.execute_reply.started":"2024-08-06T00:42:16.719003Z","shell.execute_reply":"2024-08-06T00:42:16.722911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"constants\" >\n    <h1 style=\"font-family:verdana;\"> Constants </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/mnist-in-csv'","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.72524Z","iopub.execute_input":"2024-08-06T00:42:16.725646Z","iopub.status.idle":"2024-08-06T00:42:16.731794Z","shell.execute_reply.started":"2024-08-06T00:42:16.725614Z","shell.execute_reply":"2024-08-06T00:42:16.73103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    SEED = 42\n    LEARNING_RATE : float = 1e-3\n    BATCH_SIZE : int = 64\n    WEIGHT_DECAY : float = 0.0\n    EPOCHS : int = 10\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    NUM_CLASSES = 10\n    NOISE_RATE : float = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.734103Z","iopub.execute_input":"2024-08-06T00:42:16.734481Z","iopub.status.idle":"2024-08-06T00:42:16.768433Z","shell.execute_reply.started":"2024-08-06T00:42:16.734452Z","shell.execute_reply":"2024-08-06T00:42:16.767665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"reproducibility\" >\n    <h1 style=\"font-family:verdana;\"> Reproducibility </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed : int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.769572Z","iopub.execute_input":"2024-08-06T00:42:16.769884Z","iopub.status.idle":"2024-08-06T00:42:16.776613Z","shell.execute_reply.started":"2024-08-06T00:42:16.769854Z","shell.execute_reply":"2024-08-06T00:42:16.775762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(CONFIG.SEED)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.777904Z","iopub.execute_input":"2024-08-06T00:42:16.77823Z","iopub.status.idle":"2024-08-06T00:42:16.787596Z","shell.execute_reply.started":"2024-08-06T00:42:16.778202Z","shell.execute_reply":"2024-08-06T00:42:16.786695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"data_loading\" >\n    <h1 style=\"font-family:verdana;\"> Data Loading </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"class MnistDataset(Dataset):\n\n    def __init__(self, \n        df_path : str,\n        target_col : str,\n        features_transform : Optional[Callable] = None,\n        copy_transform : Optional[Callable] = None,\n        target_transform : Optional[Callable] = None,\n    ) -> None:\n\n        super().__init__()\n\n        self.df_path = df_path\n        self.target_col = target_col\n        self.features_transform = features_transform\n        self.copy_transform = copy_transform\n        self.target_transform = target_transform\n\n        self.data = pd.read_csv(df_path)\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx : int) -> Any:\n\n        row = self.data.iloc[idx]\n        features = np.array(row.drop(self.target_col).to_list()).reshape(28,28).astype(np.uint8)\n        features_copy = features.copy()\n        target = row[self.target_col]\n\n        if self.features_transform is not None:\n            features = self.features_transform(features)\n            \n        if self.copy_transform is not None:\n            features_copy = self.copy_transform(features_copy)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return features,features_copy,target","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.788716Z","iopub.execute_input":"2024-08-06T00:42:16.789191Z","iopub.status.idle":"2024-08-06T00:42:16.798967Z","shell.execute_reply.started":"2024-08-06T00:42:16.789162Z","shell.execute_reply":"2024-08-06T00:42:16.798132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MnistDataset(df_path=os.path.join(DATA_DIR,'mnist_train.csv'),target_col='label')\ntest_dataset = MnistDataset(df_path=os.path.join(DATA_DIR,'mnist_test.csv'),target_col='label')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:16.800295Z","iopub.execute_input":"2024-08-06T00:42:16.800621Z","iopub.status.idle":"2024-08-06T00:42:22.922492Z","shell.execute_reply.started":"2024-08-06T00:42:16.800592Z","shell.execute_reply":"2024-08-06T00:42:22.92155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"utils\" >\n    <h1 style=\"font-family:verdana;\"> Utils </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"def create_dataloader(\n    train_features_transfrom : Callable,\n    train_copy_transfrom : Callable,\n    test_features_transfrom : Callable,\n    test_copy_transfrom : Callable,\n) -> tuple[DataLoader,DataLoader]:\n    \n    train_dataset = MnistDataset(\n        df_path=os.path.join(DATA_DIR,'mnist_train.csv'),\n        target_col='label',\n        features_transform=train_features_transfrom,\n        copy_transform=train_copy_transfrom\n    )\n    \n    test_dataset = MnistDataset(\n        df_path=os.path.join(DATA_DIR,'mnist_test.csv'),\n        target_col='label',\n        features_transform=test_features_transfrom,\n        copy_transform=test_copy_transfrom\n    )\n    \n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        num_workers=4,\n        prefetch_factor=2\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        prefetch_factor=2\n    )\n    \n    return train_loader,test_loader","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.92367Z","iopub.execute_input":"2024-08-06T00:42:22.92396Z","iopub.status.idle":"2024-08-06T00:42:22.931787Z","shell.execute_reply.started":"2024-08-06T00:42:22.923935Z","shell.execute_reply":"2024-08-06T00:42:22.930671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    model : nn.Module,\n    loss_fn : nn.Module,\n    train_loader : DataLoader,\n    val_loader : DataLoader,\n    learning_rate : float,\n    weight_decay : float,\n    epochs : int,\n    device : torch.device,\n    use_label : bool = False,\n    show_progress : bool = True,\n    verbose : bool = True,\n) -> pd.DataFrame:\n    \n    ### Initialize history\n    history = {\n        \"epoch\" : [],\n        \"split\" : [],\n        \"loss\" : []\n    }\n        \n    ### Move the model to the right device\n    model = model.to(device)\n        \n    ### Loss function\n    loss_fn = loss_fn.to(device)\n    \n    ### Optimizer\n    optimizer = optim.AdamW(params=model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n    \n    ### Training    \n    for epoch in range(epochs):\n        \n        ### Training Loop\n        iterator = tqdm(enumerate(train_loader),total=len(train_loader)) if show_progress else enumerate(train_loader)\n        \n        if show_progress:\n            iterator.set_description(f\"Epoch : {(epoch+1)}/{epochs}\")\n        \n        model = model.train()\n        running_loss = 0.0\n        \n        for i,(x,y,z) in iterator:\n            \n            ### Move the data to the right device\n            x,y,z = x.to(device),y.to(device),z.to(device)\n            \n            ### Forward pass\n            y_hat = model(x) if not use_label else model((x,z))\n            \n            ### Loss\n            loss = loss_fn(y_hat,y)\n            running_loss += loss.item()\n            \n            ### Zero gradient\n            optimizer.zero_grad()\n            \n            ### Backward pass\n            loss.backward()\n            \n            ### Update the network's weights\n            optimizer.step()\n            \n            ### Update the progress bar\n            if show_progress:\n                iterator.set_postfix(running_mean_loss=(running_loss / (i+1)))\n            \n        ### Update the history\n        history[\"epoch\"].append(epoch)\n        history[\"split\"].append(\"train\")\n        history[\"loss\"].append(running_loss/len(train_loader))\n            \n        if verbose:\n             message = f\"Epoch : {(epoch+1)}/{epochs},Split : train,loss={history['loss'][-1]},\"\n             print(message)\n                                \n        ### Validation Loop\n        iterator = tqdm(enumerate(val_loader),total=len(val_loader)) if show_progress else enumerate(val_loader)\n        \n        if show_progress:\n            iterator.set_description(f\"Epoch : {(epoch+1)}/{epochs}\")\n        \n        model = model.eval()\n        running_loss = 0.0\n        \n        for i,(x,y,z) in iterator:\n            \n            ### Move the data to the right device\n            x,y,z = x.to(device),y.to(device),z.to(device)\n            \n            ### Forward pass\n            y_hat = model(x) if not use_label else model((x,z))\n            \n            ### Loss\n            loss = loss_fn(y_hat,y)\n            running_loss += loss.item()\n        \n        ### Update the history\n        history[\"epoch\"].append(epoch)\n        history[\"split\"].append(\"val\")\n        history[\"loss\"].append(running_loss/len(val_loader))\n            \n        if verbose:\n            message = f\"Epoch : {(epoch+1)}/{epochs},Split : val,loss={history['loss'][-1]},\"\n            print(message)\n                \n    history = pd.DataFrame(history)\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.936661Z","iopub.execute_input":"2024-08-06T00:42:22.937187Z","iopub.status.idle":"2024-08-06T00:42:22.954007Z","shell.execute_reply.started":"2024-08-06T00:42:22.937155Z","shell.execute_reply":"2024-08-06T00:42:22.953038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Reshape(nn.Module):\n    \n    def __init__(self, new_shape : tuple):\n        super().__init__()\n        \n        self.new_shape = new_shape\n        \n    def forward(self, x : Tensor) -> Tensor:\n        B = x.size(0)\n        x = torch.reshape(x, (B,)+self.new_shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.955086Z","iopub.execute_input":"2024-08-06T00:42:22.955361Z","iopub.status.idle":"2024-08-06T00:42:22.965206Z","shell.execute_reply.started":"2024-08-06T00:42:22.955339Z","shell.execute_reply":"2024-08-06T00:42:22.964396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def knn_score(\n    train_data : tuple[np.ndarray,np.ndarray],\n    test_data : tuple[np.ndarray,np.ndarray],\n) -> float:\n    \n    x_train, y_train = train_data\n    x_test, y_test = test_data\n\n    knn = Pipeline([\n        ('scaler',StandardScaler()),\n        ('knn',KNeighborsClassifier(n_neighbors=3))\n    ])\n    knn.fit(x_train,y_train)\n\n    return knn.score(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.966453Z","iopub.execute_input":"2024-08-06T00:42:22.967052Z","iopub.status.idle":"2024-08-06T00:42:22.973694Z","shell.execute_reply.started":"2024-08-06T00:42:22.967023Z","shell.execute_reply":"2024-08-06T00:42:22.97286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(model : nn.Module, dataloader : DataLoader) -> tuple[Tensor,Tensor]:\n\n    features = []\n    targets = []\n    \n    model = model.eval()\n\n    for x,_,y in tqdm(dataloader):\n\n        x = x.to(CONFIG.DEVICE)\n\n        with torch.no_grad():\n            x = model.encoder(x)\n\n        features.append(x.cpu())\n        targets.append(y)\n\n    features = torch.cat(features).numpy()\n    targets = torch.cat(targets).numpy()\n\n    return features, targets","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.974655Z","iopub.execute_input":"2024-08-06T00:42:22.974938Z","iopub.status.idle":"2024-08-06T00:42:22.981604Z","shell.execute_reply.started":"2024-08-06T00:42:22.974916Z","shell.execute_reply":"2024-08-06T00:42:22.980818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_autoencoder(\n    img : np.array,\n    transform : Callable,\n    model : nn.Module\n):\n    fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n    img = transform(img)\n    img = img.unsqueeze(0).to(CONFIG.DEVICE)\n    \n    with torch.inference_mode():\n        decoded_img = model(img).squeeze().cpu().numpy()\n        \n    img = img[0].detach().cpu()\n    \n    if len(img.shape) == 3:\n        img = img.permute((1,2,0))\n    \n    ax1.imshow(img, cmap='gray')\n    ax2.imshow(decoded_img, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:45:05.53995Z","iopub.execute_input":"2024-08-06T00:45:05.540344Z","iopub.status.idle":"2024-08-06T00:45:05.547622Z","shell.execute_reply.started":"2024-08-06T00:45:05.540315Z","shell.execute_reply":"2024-08-06T00:45:05.546642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(images : np.ndarray,targets : np.ndarray | None = None,nrows : int = 4,ncols : int = 4) -> None:\n    \n    fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(10,10))\n    \n    k = 0\n\n    for i in range(nrows):\n        for j in range(ncols):\n\n            image = images[k]\n            \n            axes[i,j].imshow(image,cmap='gray')\n            axes[i,j].axis('off')\n            \n            if targets is not None:\n                axes[i,j].set_title(f\"Target {targets[k]}\")\n            \n            k += 1","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:22.993401Z","iopub.execute_input":"2024-08-06T00:42:22.993697Z","iopub.status.idle":"2024-08-06T00:42:23.001755Z","shell.execute_reply.started":"2024-08-06T00:42:22.993665Z","shell.execute_reply":"2024-08-06T00:42:23.000835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"visualization\" >\n    <h1 style=\"font-family:verdana;\"> Visualization </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"sample = train_dataset.data.sample(36)\nimages = sample.drop(columns=\"label\").values.reshape(-1,28,28)\nlabels = sample[\"label\"].values\nplot_images(images,labels,6,6)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:23.003056Z","iopub.execute_input":"2024-08-06T00:42:23.003408Z","iopub.status.idle":"2024-08-06T00:42:27.778368Z","shell.execute_reply.started":"2024-08-06T00:42:23.003382Z","shell.execute_reply":"2024-08-06T00:42:27.777422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"simple_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> Simple Auto-Encoders </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"1_the_architecture\" >\n    <h2 style=\"font-family:verdana;\"> The architecture </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class SimpleAutoEncoder(nn.Module):\n    \n    def __init__(self, \n        input_dim : tuple,\n        hidden_dim : int,\n        bottleneck_dim : int\n    ) -> None:\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.bottleneck_dim = bottleneck_dim\n        \n        input_dim = reduce(lambda a,b : a * b,input_dim)\n        \n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, bottleneck_dim),\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid(),\n            Reshape(self.input_dim)\n        )\n        \n    def forward(self, x : Tensor) -> Tensor:\n        \n        x = self.encoder(x)\n        x = self.decoder(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:27.779466Z","iopub.execute_input":"2024-08-06T00:42:27.779729Z","iopub.status.idle":"2024-08-06T00:42:27.790045Z","shell.execute_reply.started":"2024-08-06T00:42:27.779707Z","shell.execute_reply":"2024-08-06T00:42:27.788992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SimpleAutoEncoder(\n    input_dim=(28,28),\n    hidden_dim=128,\n    bottleneck_dim=2\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:27.79116Z","iopub.execute_input":"2024-08-06T00:42:27.791424Z","iopub.status.idle":"2024-08-06T00:42:27.824931Z","shell.execute_reply.started":"2024-08-06T00:42:27.791401Z","shell.execute_reply":"2024-08-06T00:42:27.824262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model,input_size=(32,28,28),device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:27.826037Z","iopub.execute_input":"2024-08-06T00:42:27.826355Z","iopub.status.idle":"2024-08-06T00:42:27.909886Z","shell.execute_reply.started":"2024-08-06T00:42:27.826325Z","shell.execute_reply":"2024-08-06T00:42:27.908995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"1_training\" >\n    <h2 style=\"font-family:verdana;\"> Training </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"transform = T.Compose([\n    T.ToTensor(),\n    T.Lambda(torch.squeeze)\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:27.91102Z","iopub.execute_input":"2024-08-06T00:42:27.911279Z","iopub.status.idle":"2024-08-06T00:42:27.915658Z","shell.execute_reply.started":"2024-08-06T00:42:27.911256Z","shell.execute_reply":"2024-08-06T00:42:27.914729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader,test_loader = create_dataloader(transform,transform,transform,transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:27.91677Z","iopub.execute_input":"2024-08-06T00:42:27.917069Z","iopub.status.idle":"2024-08-06T00:42:32.021305Z","shell.execute_reply.started":"2024-08-06T00:42:27.917037Z","shell.execute_reply":"2024-08-06T00:42:32.020532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(\n    model,\n    nn.MSELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=5,\n    device=CONFIG.DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:42:32.022475Z","iopub.execute_input":"2024-08-06T00:42:32.02281Z","iopub.status.idle":"2024-08-06T00:44:18.819532Z","shell.execute_reply.started":"2024-08-06T00:42:32.022781Z","shell.execute_reply":"2024-08-06T00:44:18.818196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"1_evaluation\" >\n    <h2 style=\"font-family:verdana;\"> Evaluation </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Learning curve </div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:44:18.821282Z","iopub.execute_input":"2024-08-06T00:44:18.821665Z","iopub.status.idle":"2024-08-06T00:44:19.336286Z","shell.execute_reply.started":"2024-08-06T00:44:18.821629Z","shell.execute_reply":"2024-08-06T00:44:19.334253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Original Image Vs Decoded Image </div>","metadata":{}},{"cell_type":"code","source":"test_autoencoder(\n    img=test_dataset[0][0],\n    transform=transform,\n    model=model\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:45:11.879718Z","iopub.execute_input":"2024-08-06T00:45:11.880461Z","iopub.status.idle":"2024-08-06T00:45:12.393949Z","shell.execute_reply.started":"2024-08-06T00:45:11.880428Z","shell.execute_reply":"2024-08-06T00:45:12.393103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ The reconstructed image is blurry and not the most accurate one this is due to the small size of the bottleneck layer.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"1_autoencoders_vs_pca_visualization\" >\n    <h2 style=\"font-family:verdana;\"> Auto-Encoders Vs PCA (Visualization) </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"autoencoder_train_features,autoencoder_train_targets = extract_features(model, train_loader)\nautoencoder_test_features,autoencoder_test_targets = extract_features(model, test_loader)\nautoencoder_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:45:19.004035Z","iopub.execute_input":"2024-08-06T00:45:19.004411Z","iopub.status.idle":"2024-08-06T00:45:39.250164Z","shell.execute_reply.started":"2024-08-06T00:45:19.004381Z","shell.execute_reply":"2024-08-06T00:45:39.249035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_model = Pipeline([\n    (\"scaler\",StandardScaler()),\n    (\"pca\",PCA(n_components=2))\n]).fit(train_dataset.data.drop(columns=\"label\"))\n\npca_train_features = pca_model.transform(train_dataset.data.drop(columns=\"label\"))\npca_test_features = pca_model.transform(test_dataset.data.drop(columns=\"label\"))\n\npca_train_targets = train_dataset.data[\"label\"]\npca_test_targets = test_dataset.data[\"label\"]\n\npca_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:45:46.337187Z","iopub.execute_input":"2024-08-06T00:45:46.337552Z","iopub.status.idle":"2024-08-06T00:45:49.662191Z","shell.execute_reply.started":"2024-08-06T00:45:46.337523Z","shell.execute_reply":"2024-08-06T00:45:49.66112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=autoencoder_train_features[:,0],y=autoencoder_train_features[:,1],hue=autoencoder_train_targets,palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:45:52.759407Z","iopub.execute_input":"2024-08-06T00:45:52.759776Z","iopub.status.idle":"2024-08-06T00:45:56.953375Z","shell.execute_reply.started":"2024-08-06T00:45:52.759745Z","shell.execute_reply":"2024-08-06T00:45:56.952392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=pca_train_features[:,0],y=pca_train_features[:,1],hue=pca_train_targets,palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:00.48173Z","iopub.execute_input":"2024-08-06T00:46:00.482435Z","iopub.status.idle":"2024-08-06T00:46:06.334924Z","shell.execute_reply.started":"2024-08-06T00:46:00.482401Z","shell.execute_reply":"2024-08-06T00:46:06.333813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ Autoencoder based-visualization is better as the classes are better seperated.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"1_autoencoders_vs_pca_features_quality\" >\n    <h2 style=\"font-family:verdana;\"> Auto-Encoders Vs PCA (Features Quality) </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"knn_score(\n    train_data=(autoencoder_train_features,autoencoder_train_targets),\n    test_data=(autoencoder_test_features,autoencoder_test_targets)\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:12.70514Z","iopub.execute_input":"2024-08-06T00:46:12.706047Z","iopub.status.idle":"2024-08-06T00:46:13.323415Z","shell.execute_reply.started":"2024-08-06T00:46:12.706012Z","shell.execute_reply":"2024-08-06T00:46:13.322407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_score(\n    train_data=(pca_train_features,pca_train_targets),\n    test_data=(pca_test_features,pca_test_targets)\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:17.172846Z","iopub.execute_input":"2024-08-06T00:46:17.17322Z","iopub.status.idle":"2024-08-06T00:46:17.797648Z","shell.execute_reply.started":"2024-08-06T00:46:17.173191Z","shell.execute_reply":"2024-08-06T00:46:17.796796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ The Autoencoder was able capture more information than PCA.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"1_retraining_with_bigger_bottleneck_size\" >\n    <h2 style=\"font-family:verdana;\"> Training with a bigger bottleneck size </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Training </div>","metadata":{}},{"cell_type":"code","source":"model = SimpleAutoEncoder(\n    input_dim=(28,28),\n    hidden_dim=128,\n    bottleneck_dim=64\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:23.150327Z","iopub.execute_input":"2024-08-06T00:46:23.151292Z","iopub.status.idle":"2024-08-06T00:46:23.159411Z","shell.execute_reply.started":"2024-08-06T00:46:23.15125Z","shell.execute_reply":"2024-08-06T00:46:23.158507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model,input_size=(32,28,28),device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:24.788383Z","iopub.execute_input":"2024-08-06T00:46:24.789121Z","iopub.status.idle":"2024-08-06T00:46:24.80138Z","shell.execute_reply.started":"2024-08-06T00:46:24.789074Z","shell.execute_reply":"2024-08-06T00:46:24.800472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(\n    model,\n    nn.MSELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=5,\n    device=CONFIG.DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:46:39.719762Z","iopub.execute_input":"2024-08-06T00:46:39.720162Z","iopub.status.idle":"2024-08-06T00:48:23.34134Z","shell.execute_reply.started":"2024-08-06T00:46:39.720129Z","shell.execute_reply":"2024-08-06T00:48:23.340199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Learning curve </div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:48:26.747724Z","iopub.execute_input":"2024-08-06T00:48:26.748132Z","iopub.status.idle":"2024-08-06T00:48:27.195432Z","shell.execute_reply.started":"2024-08-06T00:48:26.748101Z","shell.execute_reply":"2024-08-06T00:48:27.194616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Original Image Vs Decoded Image </div>","metadata":{}},{"cell_type":"code","source":"test_autoencoder(\n    img=test_dataset[0][0],\n    transform=transform,\n    model=model\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:48:35.452625Z","iopub.execute_input":"2024-08-06T00:48:35.453527Z","iopub.status.idle":"2024-08-06T00:48:36.07254Z","shell.execute_reply.started":"2024-08-06T00:48:35.453482Z","shell.execute_reply":"2024-08-06T00:48:36.071711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ This time the reconstructed image has a much better quality.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> The quality of the features </div>","metadata":{}},{"cell_type":"code","source":"autoencoder_train_features,autoencoder_train_targets = extract_features(model, train_loader)\nautoencoder_test_features,autoencoder_test_targets = extract_features(model, test_loader)\nautoencoder_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:49:22.865361Z","iopub.execute_input":"2024-08-06T00:49:22.866055Z","iopub.status.idle":"2024-08-06T00:49:42.951584Z","shell.execute_reply.started":"2024-08-06T00:49:22.866016Z","shell.execute_reply":"2024-08-06T00:49:42.950434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_score(\n    train_data=(autoencoder_train_features,autoencoder_train_targets),\n    test_data=(autoencoder_test_features,autoencoder_test_targets)\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:49:46.76548Z","iopub.execute_input":"2024-08-06T00:49:46.766205Z","iopub.status.idle":"2024-08-06T00:49:49.298577Z","shell.execute_reply.started":"2024-08-06T00:49:46.766169Z","shell.execute_reply":"2024-08-06T00:49:49.297592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ With a bottleneck size equals to 64,KNN reached +97% accuracy on the the test set.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"convolutional_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> Convolutional Auto-Encoders Auto-Encoders </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"2_the_architecture\" >\n    <h2 style=\"font-family:verdana;\"> The architecture </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class ConvAutoEncoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3),\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 64, kernel_size=3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, kernel_size=3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:00.757081Z","iopub.execute_input":"2024-08-06T00:50:00.757467Z","iopub.status.idle":"2024-08-06T00:50:00.76741Z","shell.execute_reply.started":"2024-08-06T00:50:00.757441Z","shell.execute_reply":"2024-08-06T00:50:00.766432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ConvAutoEncoder()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:04.233661Z","iopub.execute_input":"2024-08-06T00:50:04.234402Z","iopub.status.idle":"2024-08-06T00:50:04.243619Z","shell.execute_reply.started":"2024-08-06T00:50:04.234366Z","shell.execute_reply":"2024-08-06T00:50:04.242645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model,input_size=(32,1,28,28),device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:06.895453Z","iopub.execute_input":"2024-08-06T00:50:06.896447Z","iopub.status.idle":"2024-08-06T00:50:06.977892Z","shell.execute_reply.started":"2024-08-06T00:50:06.896411Z","shell.execute_reply":"2024-08-06T00:50:06.977021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"2_training\" >\n    <h2 style=\"font-family:verdana;\"> Training </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"transform = T.ToTensor()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:11.100074Z","iopub.execute_input":"2024-08-06T00:50:11.100895Z","iopub.status.idle":"2024-08-06T00:50:11.104998Z","shell.execute_reply.started":"2024-08-06T00:50:11.100859Z","shell.execute_reply":"2024-08-06T00:50:11.104057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader,test_loader = create_dataloader(transform,transform,transform,transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:12.881351Z","iopub.execute_input":"2024-08-06T00:50:12.882019Z","iopub.status.idle":"2024-08-06T00:50:17.236834Z","shell.execute_reply.started":"2024-08-06T00:50:12.881966Z","shell.execute_reply":"2024-08-06T00:50:17.236037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(\n    model,\n    nn.MSELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=5,\n    device=CONFIG.DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:50:21.429771Z","iopub.execute_input":"2024-08-06T00:50:21.430179Z","iopub.status.idle":"2024-08-06T00:52:09.262154Z","shell.execute_reply.started":"2024-08-06T00:50:21.430147Z","shell.execute_reply":"2024-08-06T00:52:09.261051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"2_evaluation\" >\n    <h2 style=\"font-family:verdana;\"> Evaluation </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Learning curve </div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:52:11.113394Z","iopub.execute_input":"2024-08-06T00:52:11.113802Z","iopub.status.idle":"2024-08-06T00:52:11.56226Z","shell.execute_reply.started":"2024-08-06T00:52:11.113768Z","shell.execute_reply":"2024-08-06T00:52:11.561311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Original Image Vs Decoded Image </div>","metadata":{}},{"cell_type":"code","source":"test_autoencoder(\n    img=test_dataset[0][0],\n    transform=transform,\n    model=model\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:52:18.3512Z","iopub.execute_input":"2024-08-06T00:52:18.352039Z","iopub.status.idle":"2024-08-06T00:52:18.969846Z","shell.execute_reply.started":"2024-08-06T00:52:18.351989Z","shell.execute_reply":"2024-08-06T00:52:18.968881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Features Quality </div>","metadata":{}},{"cell_type":"code","source":"autoencoder_train_features,autoencoder_train_targets = extract_features(model, train_loader)\nautoencoder_test_features,autoencoder_test_targets = extract_features(model, test_loader)\nautoencoder_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:52:31.178752Z","iopub.execute_input":"2024-08-06T00:52:31.179558Z","iopub.status.idle":"2024-08-06T00:52:51.15209Z","shell.execute_reply.started":"2024-08-06T00:52:31.179523Z","shell.execute_reply":"2024-08-06T00:52:51.150858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder_train_features = autoencoder_train_features.squeeze()\nautoencoder_test_features = autoencoder_test_features.squeeze()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:52:53.915128Z","iopub.execute_input":"2024-08-06T00:52:53.916209Z","iopub.status.idle":"2024-08-06T00:52:53.920743Z","shell.execute_reply.started":"2024-08-06T00:52:53.916167Z","shell.execute_reply":"2024-08-06T00:52:53.919648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_score(\n    train_data=(autoencoder_train_features,autoencoder_train_targets),\n    test_data=(autoencoder_test_features,autoencoder_test_targets)\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:52:56.576539Z","iopub.execute_input":"2024-08-06T00:52:56.576923Z","iopub.status.idle":"2024-08-06T00:52:58.934094Z","shell.execute_reply.started":"2024-08-06T00:52:56.576894Z","shell.execute_reply":"2024-08-06T00:52:58.933118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ With Less parameters we were able to get another auto-encoder with a similar performance.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"denoising_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> Denoising Auto-Encoders </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"3_adding_noise_to_the_input\" >\n    <h2 style=\"font-family:verdana;\"> Adding noise to the data </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class RandomNoise(nn.Module):\n\n    def __init__(self, noise_rate : float = 0.3, copy : bool = False) -> None:\n\n        super().__init__()\n\n        self.noise_rate = noise_rate\n        self.copy = copy\n\n    def forward(self, x : Tensor) -> Tensor:\n\n        if self.copy:\n            x = x.clone()\n            \n        ### Flatten the input\n        og_input = x.shape\n        x = x.flatten()\n        \n        ### Get the pixles that will be noised\n        mask = torch.rand(x.shape[0]) < self.noise_rate\n        pixles = x[mask]\n\n        ### Randomally shuffle the pixles\n        pixles = pixles[torch.randperm(pixles.shape[0])]\n\n        ### Replace the pixles in the original tensor\n        x[mask] = pixles\n        \n        ### Reshape to the original shape\n        x = x.reshape(og_input)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:05.076628Z","iopub.execute_input":"2024-08-06T00:53:05.077011Z","iopub.status.idle":"2024-08-06T00:53:05.08474Z","shell.execute_reply.started":"2024-08-06T00:53:05.076971Z","shell.execute_reply":"2024-08-06T00:53:05.083783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_random_noise(image : Tensor, noise_rate : float = 0.3) -> None:\n\n    noise = RandomNoise(noise_rate=noise_rate,copy=True)\n    x_noised = noise(image)\n\n    fig, axes = plt.subplots(1,2,figsize=(10,5))\n\n    axes[0].imshow(image,cmap='gray')\n    axes[0].set_title('Original Image')\n\n    axes[1].imshow(x_noised,cmap='gray')\n    axes[1].set_title('Noised Image')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:06.726001Z","iopub.execute_input":"2024-08-06T00:53:06.726937Z","iopub.status.idle":"2024-08-06T00:53:06.733009Z","shell.execute_reply.started":"2024-08-06T00:53:06.726904Z","shell.execute_reply":"2024-08-06T00:53:06.732128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_random_noise(\n    image=torch.tensor(train_dataset[0][0]),\n    noise_rate=0.3\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:09.783784Z","iopub.execute_input":"2024-08-06T00:53:09.784183Z","iopub.status.idle":"2024-08-06T00:53:10.387526Z","shell.execute_reply.started":"2024-08-06T00:53:09.784153Z","shell.execute_reply":"2024-08-06T00:53:10.386635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"3_training\" >\n    <h2 style=\"font-family:verdana;\"> Training </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"transform = T.Compose([\n    T.ToTensor(),\n    T.Lambda(torch.squeeze)\n])\n\ntrain_features_transform = T.Compose([\n    T.ToTensor(),\n    RandomNoise(noise_rate=CONFIG.NOISE_RATE,copy=True),\n    T.Lambda(torch.squeeze)\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:13.572881Z","iopub.execute_input":"2024-08-06T00:53:13.573775Z","iopub.status.idle":"2024-08-06T00:53:13.57922Z","shell.execute_reply.started":"2024-08-06T00:53:13.573734Z","shell.execute_reply":"2024-08-06T00:53:13.578238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader,test_loader = create_dataloader(train_features_transform,transform,transform,transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:16.545326Z","iopub.execute_input":"2024-08-06T00:53:16.545706Z","iopub.status.idle":"2024-08-06T00:53:20.969022Z","shell.execute_reply.started":"2024-08-06T00:53:16.545677Z","shell.execute_reply":"2024-08-06T00:53:20.968022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SimpleAutoEncoder(\n    input_dim=(28,28),\n    hidden_dim=128,\n    bottleneck_dim=64\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:22.59082Z","iopub.execute_input":"2024-08-06T00:53:22.591216Z","iopub.status.idle":"2024-08-06T00:53:22.598542Z","shell.execute_reply.started":"2024-08-06T00:53:22.591184Z","shell.execute_reply":"2024-08-06T00:53:22.597554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(\n    model,\n    nn.MSELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=10,\n    device=CONFIG.DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:53:24.63481Z","iopub.execute_input":"2024-08-06T00:53:24.635665Z","iopub.status.idle":"2024-08-06T00:57:29.177149Z","shell.execute_reply.started":"2024-08-06T00:53:24.635633Z","shell.execute_reply":"2024-08-06T00:57:29.176064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"3_evaluation\" >\n    <h2 style=\"font-family:verdana;\"> Evaluation </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Learning curve </div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:32.339477Z","iopub.execute_input":"2024-08-06T00:57:32.340251Z","iopub.status.idle":"2024-08-06T00:57:32.726124Z","shell.execute_reply.started":"2024-08-06T00:57:32.340213Z","shell.execute_reply":"2024-08-06T00:57:32.725247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:verdana;font-size:18px;\"> Noised Vs Reconstructed Image </div>","metadata":{}},{"cell_type":"code","source":"test_autoencoder(\n    img=test_dataset[0][0],\n    transform=train_features_transform,\n    model=model\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:36.443287Z","iopub.execute_input":"2024-08-06T00:57:36.443693Z","iopub.status.idle":"2024-08-06T00:57:37.3179Z","shell.execute_reply.started":"2024-08-06T00:57:36.443661Z","shell.execute_reply":"2024-08-06T00:57:37.316996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ The autoencoder was able to remove the noise introduced on the input image.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"variational_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> Variational Autoencoder </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"4_the_architecture\" >\n    <h2 style=\"font-family:verdana;\"> The Architecture </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class VariationalAutoEncoder(nn.Module):\n    \n    def __init__(self, \n        input_dim : tuple,\n        hidden_dim : int,\n        bottleneck_dim : int\n    ) -> None:\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.bottleneck_dim = bottleneck_dim\n        \n        input_dim = reduce(lambda a,b : a * b,input_dim)\n        \n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 2 * bottleneck_dim),\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid(),\n            Reshape(self.input_dim)\n        )\n        \n    def forward(self, x : Tensor) -> Tensor:\n        \n        x = self.encoder(x)\n        mean,log_var = x[:,:self.bottleneck_dim],x[:,self.bottleneck_dim:]\n        x = self.reparameterization(mean, torch.exp(0.5 * log_var))\n        x = self.decoder(x)\n        \n        return x,mean,log_var\n    \n    def generate(self,\n        batch_size : int,\n    ) -> Tensor:\n        \n        x = torch.randn(batch_size, self.bottleneck_dim).to(CONFIG.DEVICE)\n\n        with torch.inference_mode():\n            generated_images = self.decoder(x).detach().cpu()\n        \n        return generated_images\n    \n    def reparameterization(self, mean : Tensor, var : Tensor) -> Tensor:\n        epsilon = torch.randn_like(var).to(var.device)    \n        z = mean + var * epsilon                    \n        return z","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:10:01.312283Z","iopub.execute_input":"2024-08-06T01:10:01.312972Z","iopub.status.idle":"2024-08-06T01:10:01.325167Z","shell.execute_reply.started":"2024-08-06T01:10:01.312938Z","shell.execute_reply":"2024-08-06T01:10:01.324237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VariationalAutoEncoder(\n    input_dim=(28,28),\n    hidden_dim=384,\n    bottleneck_dim=64\n).to(CONFIG.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:11:41.354384Z","iopub.execute_input":"2024-08-06T01:11:41.35525Z","iopub.status.idle":"2024-08-06T01:11:41.374756Z","shell.execute_reply.started":"2024-08-06T01:11:41.355207Z","shell.execute_reply":"2024-08-06T01:11:41.373885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model, input_size=(32,28,28),device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:46.474826Z","iopub.execute_input":"2024-08-06T00:57:46.47565Z","iopub.status.idle":"2024-08-06T00:57:46.494069Z","shell.execute_reply.started":"2024-08-06T00:57:46.475618Z","shell.execute_reply":"2024-08-06T00:57:46.493137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"4_the_loss_function\" >\n    <h2 style=\"font-family:verdana;\"> The loss function </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class VAELoss(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n    def forward(self,inputs : tuple[Tensor,Tensor,Tensor],targets : Tensor) -> Tensor:\n        \n        x,mean,log_var = inputs\n        \n        reconstruction_loss = F.binary_cross_entropy(x,targets,reduction='sum')\n        kld_loss = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n        \n        return reconstruction_loss + kld_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:50.21396Z","iopub.execute_input":"2024-08-06T00:57:50.214812Z","iopub.status.idle":"2024-08-06T00:57:50.220904Z","shell.execute_reply.started":"2024-08-06T00:57:50.214781Z","shell.execute_reply":"2024-08-06T00:57:50.219788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"4_training\" >\n    <h2 style=\"font-family:verdana;\"> Training </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"transform = T.Compose([\n    T.ToTensor(),\n    T.Lambda(torch.squeeze)\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:52.937052Z","iopub.execute_input":"2024-08-06T00:57:52.93793Z","iopub.status.idle":"2024-08-06T00:57:52.942329Z","shell.execute_reply.started":"2024-08-06T00:57:52.937896Z","shell.execute_reply":"2024-08-06T00:57:52.941339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader,test_loader = create_dataloader(transform,transform,transform,transform)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:54.383537Z","iopub.execute_input":"2024-08-06T00:57:54.384218Z","iopub.status.idle":"2024-08-06T00:57:58.35877Z","shell.execute_reply.started":"2024-08-06T00:57:54.384188Z","shell.execute_reply":"2024-08-06T00:57:58.357958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = train(\n    model,\n    VAELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=30,\n    device=CONFIG.DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:57:58.360587Z","iopub.execute_input":"2024-08-06T00:57:58.361391Z","iopub.status.idle":"2024-08-06T01:08:44.406646Z","shell.execute_reply.started":"2024-08-06T00:57:58.361352Z","shell.execute_reply":"2024-08-06T01:08:44.405299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"4_evaluation\" >\n    <h2 style=\"font-family:verdana;\"> Evaluation </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:09:33.070199Z","iopub.execute_input":"2024-08-06T01:09:33.071309Z","iopub.status.idle":"2024-08-06T01:09:33.522941Z","shell.execute_reply.started":"2024-08-06T01:09:33.071258Z","shell.execute_reply":"2024-08-06T01:09:33.521864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"4_generating_synthetic_data\" >\n    <h2 style=\"font-family:verdana;\"> Generating Synthetic Images </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"generated_images = model.generate(batch_size=16)\nplot_images(generated_images)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:11:52.916884Z","iopub.execute_input":"2024-08-06T01:11:52.917948Z","iopub.status.idle":"2024-08-06T01:11:54.66051Z","shell.execute_reply.started":"2024-08-06T01:11:52.917913Z","shell.execute_reply":"2024-08-06T01:11:54.659635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ We were able to generate good quality synthetic images but we have no control over the number that the generated images represent.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"conditional_variational_auto_encoders\" >\n    <h1 style=\"font-family:verdana;\"> Conditional Variational Autoencoder </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"5_the_architecture\" >\n    <h2 style=\"font-family:verdana;\"> The architecture </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class ConditionalVariationalAutoEncoder(nn.Module):\n    \n    def __init__(self, \n        input_dim : tuple,\n        hidden_dim : int,\n        bottleneck_dim : int,\n        num_classes : int\n    ) -> None:\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.bottleneck_dim = bottleneck_dim\n        self.num_classes = num_classes\n        self.input_dim_flat = reduce(lambda a,b : a * b,input_dim)\n        \n        self.one_hot = nn.Parameter(data=torch.eye(num_classes), requires_grad=False)\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(self.input_dim_flat + num_classes, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 2 * bottleneck_dim),\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim + num_classes, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, self.input_dim_flat),\n            nn.Sigmoid(),\n            Reshape(self.input_dim)\n        )\n        \n    def forward(self, x : tuple[Tensor,Tensor]) -> Tensor:\n        \n        img,label = x\n        \n        ### Encoder\n        label = self.one_hot[label]\n        img = torch.flatten(img,start_dim=1)\n        x = torch.cat([img,label],dim=-1)\n        x = self.encoder(x)\n        mean,log_var = x[:,:self.bottleneck_dim],x[:,self.bottleneck_dim:]\n        \n        ### Rreparameterization\n        x = self.reparameterization(mean, torch.exp(0.5 * log_var))\n        \n        ### Decoding\n        x = torch.cat((x, label), dim=-1)\n        x = self.decoder(x)\n        \n        return x,mean,log_var\n    \n    def generate(self,\n        batch_size : int,\n        labels : int | Tensor\n    ) -> Tensor:\n        \n        if isinstance(labels,int):\n            labels = torch.ones(batch_size) * labels\n        else:\n            if labels.size(0) != batch_size:\n                raise Exception(f\"labels.size(0) = {labels.size(0)} is different than batch_size={batch_size}\")\n                \n        noise = torch.randn(batch_size, self.bottleneck_dim).to(CONFIG.DEVICE)\n        labels = self.one_hot[labels.long().to(CONFIG.DEVICE)]\n        x = torch.cat([noise,labels],dim=-1)\n        \n        with torch.inference_mode():\n            generated_images = self.decoder(x).detach().cpu()\n        \n        return generated_images\n    \n    def reparameterization(self, mean : Tensor, var : Tensor) -> Tensor:\n        epsilon = torch.randn_like(var).to(var.device)    \n        z = mean + var * epsilon                    \n        return z","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:12:09.006241Z","iopub.execute_input":"2024-08-06T01:12:09.006632Z","iopub.status.idle":"2024-08-06T01:12:09.0235Z","shell.execute_reply.started":"2024-08-06T01:12:09.006601Z","shell.execute_reply":"2024-08-06T01:12:09.02229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ConditionalVariationalAutoEncoder(\n    input_dim=(28,28),\n    hidden_dim=384,\n    bottleneck_dim=64,\n    num_classes=CONFIG.NUM_CLASSES\n).to(CONFIG.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:12:13.992968Z","iopub.execute_input":"2024-08-06T01:12:13.993755Z","iopub.status.idle":"2024-08-06T01:12:14.011314Z","shell.execute_reply.started":"2024-08-06T01:12:13.993719Z","shell.execute_reply":"2024-08-06T01:12:14.010329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = torch.zeros(32,28,28)\nlabels = torch.zeros(32).long()\nsummary(model,input_data={ \"x\" : (images,labels) },device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:12:15.653576Z","iopub.execute_input":"2024-08-06T01:12:15.654343Z","iopub.status.idle":"2024-08-06T01:12:15.671795Z","shell.execute_reply.started":"2024-08-06T01:12:15.654307Z","shell.execute_reply":"2024-08-06T01:12:15.671035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"5_training\" >\n    <h2 style=\"font-family:verdana;\"> Training </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"history = train(\n    model,\n    VAELoss(),\n    train_loader,\n    test_loader,\n    learning_rate=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n    epochs=20,\n    device=CONFIG.DEVICE,\n    use_label=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:12:17.804382Z","iopub.execute_input":"2024-08-06T01:12:17.804736Z","iopub.status.idle":"2024-08-06T01:19:24.484047Z","shell.execute_reply.started":"2024-08-06T01:12:17.80471Z","shell.execute_reply":"2024-08-06T01:19:24.482897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"5_evaluation\" >\n    <h2 style=\"font-family:verdana;\"> Evaluation </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=history,x=\"epoch\",y=\"loss\",hue=\"split\",palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:19:29.522443Z","iopub.execute_input":"2024-08-06T01:19:29.522823Z","iopub.status.idle":"2024-08-06T01:19:29.964924Z","shell.execute_reply.started":"2024-08-06T01:19:29.522794Z","shell.execute_reply":"2024-08-06T01:19:29.964051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"5_generating_synthetic_data\" >\n    <h2 style=\"font-family:verdana;\"> Generating Synthetic Images </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"generated_images = model.generate(16,9).numpy()\nplot_images(generated_images)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:19:35.308452Z","iopub.execute_input":"2024-08-06T01:19:35.308812Z","iopub.status.idle":"2024-08-06T01:19:36.993616Z","shell.execute_reply.started":"2024-08-06T01:19:35.308786Z","shell.execute_reply":"2024-08-06T01:19:36.992719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = torch.randint(low=0,high=CONFIG.NUM_CLASSES,size=(16,))\ngenerated_images = model.generate(16,labels).numpy()\nplot_images(generated_images)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:19:40.995219Z","iopub.execute_input":"2024-08-06T01:19:40.995953Z","iopub.status.idle":"2024-08-06T01:19:43.073678Z","shell.execute_reply.started":"2024-08-06T01:19:40.995922Z","shell.execute_reply":"2024-08-06T01:19:43.072713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:verdana; line-height: 1.7em;\">\n    ðŸ“Œ Now we can control the number that our generated images represent.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"conclusion\" >\n    <h1 style=\"font-family:verdana;\"> Conclusion </h1>\n</div>\n\n<div style=\"font-size: 1rem;font-family:verdana;\" >\nIn this notebook, we explored various types of autoencoders and applied them to the MNIST dataset. We began with simple autoencoders and gradually progressed to more complex architectures like convolutional, denoising, variational, and conditional variational autoencoders. Each type of autoencoder has unique strengths and applications:\n</div><br/>\n\n1. **Simple Autoencoders** : Effective for basic feature extraction and dimensionality reduction.\n2. **Convolutional Autoencoders** : Excellent for image-related tasks due to their ability to capture spatial hierarchies.\n3. **Denoising Autoencoders** : Improve robustness and can clean noisy data.\n4. **Variational Autoencoders** : Enable generation of new data samples and facilitate probabilistic interpretation.\n5. **Conditional Variational Autoencoders** : Allow for controlled generation based on additional context or labels.\n","metadata":{}},{"cell_type":"markdown","source":"<div id=\"thank_you\" >\n    <h1 style=\"font-family:verdana;\"> Thank you :) </h1>\n</div>","metadata":{}}]}